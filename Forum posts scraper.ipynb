{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de7dafc",
   "metadata": {},
   "source": [
    "## Importing libraries we will use\n",
    "### Pandas for dataframes, numpy for some plotting, web scraping tools, file creating tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09aa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime, timedelta\n",
    "from itertools import islice\n",
    "import re\n",
    "import docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09616851",
   "metadata": {},
   "source": [
    "## This will help me clean up HTML text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218d82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CLEANR = re.compile('<.*?>') \n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf6322",
   "metadata": {},
   "source": [
    "We will be scraping the following forum thread, https://forums.somethingawful.com/showthread.phpthreadid=3550307&userid=0&perpage=40&pagenumber=1\n",
    ", it's a choose your own adventure game where the creator posits a scenario, people vote on the action they want to take \n",
    "and the author continues the story based on that\n",
    "\n",
    "The game has been running for over 9 years, and hundreds of people have been involved in the voting. The forum lacks both a good search functionality and statistics regarding the posting.\n",
    "\n",
    "Players and the writer have asked me for different stats, so this analysis will be immediately used by real people other than me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569e100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rindex(lst, value):\n",
    "    lst.reverse()\n",
    "    i = lst.index(value)\n",
    "    lst.reverse()\n",
    "    return len(lst) - i - 1\n",
    "def nth_index(iterable, value, n):\n",
    "    matches = (idx for idx, val in enumerate(iterable) if val == value)\n",
    "    return next(islice(matches, n-1, n), None)\n",
    "def find_nth_occurrence(l, value, n):\n",
    "    indices = [index for index, item in enumerate(l)\n",
    "               if item == value]\n",
    "    if len(indices) < n:\n",
    "        return -1\n",
    "    return indices[n - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72e4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving today's date\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735f5e7",
   "metadata": {},
   "source": [
    "In order to automate the scraping we need to know how many pages there are,as it's continuously increasing\n",
    "We will use the \"last page\" link in the first page to obtain that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4471974d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n"
     ]
    }
   ],
   "source": [
    "LitThread=\"https://forums.somethingawful.com/showthread.php?threadid=\"\n",
    "ParadiseLost=\"https://forums.somethingawful.com/showthread.php?threadid=3550307&userid=0&perpage=40&pagenumber=\"\n",
    "response=requests.get(\"https://forums.somethingawful.com/showthread.php?threadid=3550307&userid=0&perpage=40&pagenumber=1\")\n",
    "MainPage=BeautifulSoup(response.text)\n",
    "LastpageLink=MainPage.find(\"a\",title=\"Last page\")\n",
    "LastPage=str(LastpageLink)\n",
    "LastPageNumber=int(LastPage[LastPage.rfind(\"=\")+13:LastPage.rfind(\" \")])\n",
    "print(LastPageNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e3390",
   "metadata": {},
   "source": [
    "Scraping all ~7700 pages would take too long, so I'll use a smaller sample.\n",
    "To scrape the whole thing all we need to change is the range in the for to \"LastPageNumber\"\n",
    "Scraping the \"threadlength\" pages of the thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10318388",
   "metadata": {},
   "source": [
    "At first I was saving all pages in a list, and then reiterating through it to get the data, then i realized it wasn't as efficient as it could be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02060ae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#legacy\n",
    "#Pages=[]\n",
    "#print(Pages)\n",
    "#threadlength=1\n",
    "#for i in range(threadlength):\n",
    "# Response=requests.get(\"https://forums.somethingawful.com/showthread.php?threadid=3550307&userid=0&perpage=40&pagenumber=\"+str(j))\n",
    "# pageTemp=BeautifulSoup(Response.text)\n",
    "# Pages.append(pageTemp)\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5030ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#legacy\n",
    "#Creating a Dictionary for every posts, it includes the username,the date on which the post was realized and the raw content\n",
    "#PostsDictionary={\"User_Name\":[],\"Date\":[],\"Post_Content\":[]}\n",
    "#postsperpage=42\n",
    "#for pagenumber in range(len(Pages)):\n",
    " #   print(pagenumber)\n",
    " #   for i in range(postsperpage):\n",
    "   #     username=str(Pages[pagenumber].find_all(\"td\",class_=\"userinfo\")[i].find(\"dt\"))\n",
    "    #    usernameclean=username[username.find(\">\")+1:username.find(\"<\",1)]\n",
    "    #    if usernameclean!=\"Adbot\":\n",
    "     #       PostsDictionary[\"User_Name\"].append(usernameclean)\n",
    "      #      date=str(Pages[pagenumber].find_all(\"td\",class_=\"postdate\")[i])\n",
    "      #      dateofpost=pd.to_datetime(str(date[date.rfind(\"a>\")+3:date.rfind(\"a>\")+21]))\n",
    "      #      PostsDictionary[\"Date\"].append(dateofpost)\n",
    "       #     text=str(Pages[pagenumber].find_all(\"td\",class_=\"postbody\")[i])\n",
    "       #     PostsDictionary[\"Post_Content\"].append(text)\n",
    "           #  text=str(pageTemp.find_all(\"td\",class_=\"postbody\")[k])\n",
    "            # PostsDictionary=[\"Post_Content\"].append(usernameclean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09ec01",
   "metadata": {},
   "source": [
    "So what I ended up doing was scraping each page and obtainign the data, that way I don't need to save the pages and only iterate through all of them once\n",
    "\n",
    "Scraping each page\n",
    "Creating a Dictionary for every posts, it includes \n",
    "\n",
    "a)Username\n",
    "\n",
    "b)Date on which the post was realized\n",
    "\n",
    "c)Text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d6d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "Scraping page 40\n",
      "Scraping page 41\n",
      "Scraping page 42\n",
      "Scraping page 43\n",
      "Scraping page 44\n",
      "Scraping page 45\n",
      "Scraping page 46\n",
      "Scraping page 47\n",
      "Scraping page 48\n",
      "Scraping page 49\n",
      "Scraping page 50\n",
      "Scraping page 51\n",
      "Scraping page 52\n",
      "Scraping page 53\n",
      "Scraping page 54\n",
      "Scraping page 55\n",
      "Scraping page 56\n",
      "Scraping page 57\n",
      "Scraping page 58\n",
      "Scraping page 59\n",
      "Scraping page 60\n",
      "Scraping page 61\n",
      "Scraping page 62\n",
      "Scraping page 63\n",
      "Scraping page 64\n",
      "Scraping page 65\n",
      "Scraping page 66\n",
      "Scraping page 67\n",
      "Scraping page 68\n",
      "Scraping page 69\n",
      "Scraping page 70\n",
      "Scraping page 71\n",
      "Scraping page 72\n",
      "Scraping page 73\n",
      "Scraping page 74\n",
      "Scraping page 75\n",
      "Scraping page 76\n",
      "Scraping page 77\n",
      "Scraping page 78\n",
      "Scraping page 79\n",
      "Scraping page 80\n",
      "Scraping page 81\n",
      "Scraping page 82\n",
      "Scraping page 83\n",
      "Scraping page 84\n",
      "Scraping page 85\n",
      "Scraping page 86\n",
      "Scraping page 87\n",
      "Scraping page 88\n",
      "Scraping page 89\n",
      "Scraping page 90\n",
      "Scraping page 91\n",
      "Scraping page 92\n",
      "Scraping page 93\n",
      "Scraping page 94\n",
      "Scraping page 95\n",
      "Scraping page 96\n",
      "Scraping page 97\n",
      "Scraping page 98\n",
      "Scraping page 99\n",
      "Scraping page 100\n",
      "Scraping page 101\n",
      "Scraping page 102\n",
      "Scraping page 103\n",
      "Scraping page 104\n",
      "Scraping page 105\n",
      "Scraping page 106\n",
      "Scraping page 107\n",
      "Scraping page 108\n",
      "Scraping page 109\n",
      "Scraping page 110\n",
      "Scraping page 111\n",
      "Scraping page 112\n",
      "Scraping page 113\n",
      "Scraping page 114\n",
      "Scraping page 115\n",
      "Scraping page 116\n",
      "Scraping page 117\n",
      "Scraping page 118\n",
      "Scraping page 119\n",
      "Scraping page 120\n",
      "Scraping page 121\n",
      "Scraping page 122\n",
      "Scraping page 123\n",
      "Scraping page 124\n",
      "Scraping page 125\n",
      "Scraping page 126\n",
      "Scraping page 127\n",
      "Scraping page 128\n",
      "Scraping page 129\n",
      "Scraping page 130\n",
      "Scraping page 131\n",
      "Scraping page 132\n",
      "Scraping page 133\n",
      "Scraping page 134\n",
      "Scraping page 135\n",
      "Scraping page 136\n",
      "Scraping page 137\n",
      "Scraping page 138\n",
      "Scraping page 139\n",
      "Scraping page 140\n",
      "Scraping page 141\n",
      "Scraping page 142\n",
      "Scraping page 143\n",
      "Scraping page 144\n",
      "Scraping page 145\n",
      "Scraping page 146\n",
      "Scraping page 147\n",
      "Scraping page 148\n",
      "Scraping page 149\n",
      "Scraping page 150\n",
      "Scraping page 151\n",
      "Scraping page 152\n",
      "Scraping page 153\n",
      "Scraping page 154\n",
      "Scraping page 155\n",
      "Scraping page 156\n",
      "Scraping page 157\n",
      "Scraping page 158\n",
      "Scraping page 159\n",
      "Scraping page 160\n",
      "Scraping page 161\n",
      "Scraping page 162\n",
      "Scraping page 163\n",
      "Scraping page 164\n",
      "Scraping page 165\n",
      "Scraping page 166\n",
      "Scraping page 167\n",
      "Scraping page 168\n",
      "Scraping page 169\n",
      "Scraping page 170\n",
      "Scraping page 171\n",
      "Scraping page 172\n",
      "Scraping page 173\n",
      "Scraping page 174\n",
      "Scraping page 175\n",
      "Scraping page 176\n",
      "Scraping page 177\n",
      "Scraping page 178\n",
      "Scraping page 179\n",
      "Scraping page 180\n",
      "Scraping page 181\n",
      "Scraping page 182\n",
      "Scraping page 183\n",
      "Scraping page 184\n",
      "Scraping page 185\n",
      "Scraping page 186\n",
      "Scraping page 187\n",
      "Scraping page 188\n",
      "Scraping page 189\n",
      "Scraping page 190\n",
      "Scraping page 191\n",
      "Scraping page 192\n",
      "Scraping page 193\n",
      "Scraping page 194\n",
      "Scraping page 195\n",
      "Scraping page 196\n",
      "Scraping page 197\n",
      "Scraping page 198\n",
      "Scraping page 199\n",
      "Scraping page 200\n",
      "Scraping page 201\n",
      "Scraping page 202\n",
      "Scraping page 203\n",
      "Scraping page 204\n",
      "Scraping page 205\n",
      "Scraping page 206\n",
      "Scraping page 207\n",
      "Scraping page 208\n",
      "Scraping page 209\n",
      "Scraping page 210\n",
      "Scraping page 211\n",
      "Scraping page 212\n",
      "Scraping page 213\n",
      "Scraping page 214\n",
      "Scraping page 215\n",
      "Scraping page 216\n",
      "Scraping page 217\n",
      "Scraping page 218\n",
      "Scraping page 219\n",
      "Scraping page 220\n",
      "Scraping page 221\n",
      "Scraping page 222\n",
      "Scraping page 223\n",
      "Scraping page 224\n",
      "Scraping page 225\n",
      "Scraping page 226\n",
      "Scraping page 227\n",
      "Scraping page 228\n",
      "Scraping page 229\n",
      "Scraping page 230\n",
      "Scraping page 231\n",
      "Scraping page 232\n",
      "Scraping page 233\n",
      "Scraping page 234\n",
      "Scraping page 235\n",
      "Scraping page 236\n",
      "Scraping page 237\n",
      "Scraping page 238\n",
      "Scraping page 239\n",
      "Scraping page 240\n",
      "Scraping page 241\n",
      "Scraping page 242\n",
      "Scraping page 243\n",
      "Scraping page 244\n",
      "Scraping page 245\n",
      "Scraping page 246\n",
      "Scraping page 247\n",
      "Scraping page 248\n",
      "Scraping page 249\n",
      "Scraping page 250\n",
      "Scraping page 251\n",
      "Scraping page 252\n",
      "Scraping page 253\n",
      "Scraping page 254\n",
      "Scraping page 255\n",
      "Scraping page 256\n",
      "Scraping page 257\n",
      "Scraping page 258\n",
      "Scraping page 259\n",
      "Scraping page 260\n",
      "Scraping page 261\n",
      "Scraping page 262\n",
      "Scraping page 263\n",
      "Scraping page 264\n",
      "Scraping page 265\n",
      "Scraping page 266\n",
      "Scraping page 267\n",
      "Scraping page 268\n",
      "Scraping page 269\n",
      "Scraping page 270\n",
      "Scraping page 271\n",
      "Scraping page 272\n",
      "Scraping page 273\n",
      "Scraping page 274\n",
      "Scraping page 275\n",
      "Scraping page 276\n",
      "Scraping page 277\n",
      "Scraping page 278\n",
      "Scraping page 279\n",
      "Scraping page 280\n",
      "Scraping page 281\n",
      "Scraping page 282\n",
      "Scraping page 283\n",
      "Scraping page 284\n",
      "Scraping page 285\n",
      "Scraping page 286\n",
      "Scraping page 287\n",
      "Scraping page 288\n",
      "Scraping page 289\n",
      "Scraping page 290\n",
      "Scraping page 291\n",
      "Scraping page 292\n",
      "Scraping page 293\n",
      "Scraping page 294\n",
      "Scraping page 295\n",
      "Scraping page 296\n",
      "Scraping page 297\n",
      "Scraping page 298\n",
      "Scraping page 299\n",
      "Scraping page 300\n",
      "Scraping page 301\n",
      "Scraping page 302\n",
      "Scraping page 303\n",
      "Scraping page 304\n",
      "Scraping page 305\n",
      "Scraping page 306\n",
      "Scraping page 307\n",
      "Scraping page 308\n",
      "Scraping page 309\n",
      "Scraping page 310\n",
      "Scraping page 311\n",
      "Scraping page 312\n",
      "Scraping page 313\n",
      "Scraping page 314\n",
      "Scraping page 315\n",
      "Scraping page 316\n",
      "Scraping page 317\n",
      "Scraping page 318\n",
      "Scraping page 319\n",
      "Scraping page 320\n",
      "Scraping page 321\n",
      "Scraping page 322\n",
      "Scraping page 323\n",
      "Scraping page 324\n",
      "Scraping page 325\n",
      "Scraping page 326\n",
      "Scraping page 327\n",
      "Scraping page 328\n",
      "Scraping page 329\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    415\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1309\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    415\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1309\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12200\\1914662575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scraping page \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mResponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://forums.somethingawful.com/showthread.php?threadid=3643994&userid=0&perpage=40&pagenumber=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mpageTemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpostsperpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'Se ha forzado la interrupción de una conexión existente por el host remoto', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "PostsDictionary={\"User_Name\":[],\"Date\":[],\"Post_Content\":[],\"Post_Words\":[]}\n",
    "postsperpage=42 #We know there are 40 user posts in each page, and 2 ads which are counted as posts\n",
    "for i in range(626):\n",
    "    j=i+1\n",
    "    print(\"Scraping page \"+str(j))\n",
    "    Response=requests.get(\"https://forums.somethingawful.com/showthread.php?threadid=3643994&userid=0&perpage=40&pagenumber=\"+str(j))\n",
    "    pageTemp=BeautifulSoup(Response.text)\n",
    "    for k in range(postsperpage):\n",
    "        username=str(pageTemp.find_all(\"td\",class_=\"userinfo\")[k].find(\"dt\"))\n",
    "        usernameclean=username[username.find(\">\")+1:username.find(\"<\",1)]\n",
    "        if usernameclean!=\"Adbot\": #name of the user that posts the ads previously mentioned\n",
    "            PostsDictionary[\"User_Name\"].append(usernameclean)\n",
    "            date=str(pageTemp.find_all(\"td\",class_=\"postdate\")[k])\n",
    "            dateofpost=pd.to_datetime(str(date[date.rfind(\"a>\")+3:date.rfind(\"a>\")+21]))\n",
    "            PostsDictionary[\"Date\"].append(dateofpost)\n",
    "            text=str(pageTemp.find_all(\"td\",class_=\"postbody\")[k])\n",
    "            text=cleanhtml(text)\n",
    "            text=text.replace(\"\\n\",\"\")\n",
    "            PostsDictionary[\"Post_Content\"].append(text)\n",
    "            PostsDictionary[\"Post_Words\"].append(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Response=requests.get(LitThread+str(j))\n",
    "pageTemp=BeautifulSoup(Response.text)\n",
    "#pageTemp.find_all(\"td\",class_=\"userinfo\")\n",
    "pageTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f884d37",
   "metadata": {},
   "source": [
    "Transforming into a dataframe for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b70dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFposts=pd.DataFrame(PostsDictionary)\n",
    "DFposts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f5ed8",
   "metadata": {},
   "source": [
    "## Creating a list of unique posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueUsersList=[]\n",
    "for item in PostsDictionary[\"User_Name\"]: \n",
    "    if item not in UniqueUsersList: \n",
    "        UniqueUsersList.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf721f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Series(PostsDictionary[\"User_Name\"]).value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab02a40",
   "metadata": {},
   "source": [
    "## Now I'm going to create aggregated data for each user, not on a post by post basis\n",
    "\n",
    "Includes\n",
    "\n",
    "a)User name\n",
    "\n",
    "b)Date on which they FIRST posted\n",
    "\n",
    "c)Number of times they've posted\n",
    "\n",
    "d)Posts per day. Begin date: first post's date. End date: Latest date on which somebody posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0be9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserData={\"User_Name\":[],\"First_Post\":[],\"Total_Posts\":[],\"Post_Rate\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2925f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filling that dictionary with the names\"\n",
    "UserData[\"User_Name\"].clear()\n",
    "for item in PostsDictionary[\"User_Name\"]: \n",
    "    if item not in UniqueUsersList:\n",
    "        UniqueUsersList.append(item) \n",
    "for user in UniqueUsersList:\n",
    "    UserData[\"User_Name\"].append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling that dictionary with the date of their first post\n",
    "for user in UserData[\"User_Name\"]: \n",
    "   firstappearance=PostsDictionary[\"User_Name\"].index(user) #finding the first apppearance of a poster in the PostsDictionary\n",
    "   UserData[\"First_Post\"].append(PostsDictionary[\"Date\"][firstappearance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the dictionary with total posts\n",
    "totalposts=pd.Series(PostsDictionary[\"User_Name\"]).value_counts()\n",
    "for user in UserData[\"User_Name\"]:\n",
    "    UserData[\"Total_Posts\"].append(totalposts[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3645ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestPostDate=max(PostsDictionary[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserData[\"Post_Rate\"].clear()\n",
    "for i in range(len(UserData[\"User_Name\"])):\n",
    "    TimeSincePost=str((LatestPostDate-UserData[\"First_Post\"][i]))\n",
    "    DaysSincePost=int(TimeSincePost[0:TimeSincePost.find(\"days\")])\n",
    "    if DaysSincePost==0:\n",
    "        DaysSincePost=1\n",
    "        UserData[\"Post_Rate\"].append(UserData[\"Total_Posts\"][i]/DaysSincePost)\n",
    "    else:\n",
    "        UserData[\"Post_Rate\"].append(UserData[\"Total_Posts\"][i]/DaysSincePost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6759fd1",
   "metadata": {},
   "source": [
    "### Transforming the userdata dictionary into a Datrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFusers=pd.DataFrame(UserData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c42a4c",
   "metadata": {},
   "source": [
    "#### Adding a new column wiht the totla and average amount of words per post for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95de33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Words=[]\n",
    "for user in UniqueUsersList:\n",
    "    mask1=DFposts[\"User_Name\"]==user\n",
    "    totalwords=DFposts[\"Post_Words\"][mask1].sum()\n",
    "    Total_Words.append(totalwords)\n",
    "DFusers[\"Total_Words\"]=Total_Words\n",
    "DFusers[\"Average_Words\"]=DFusers[\"Total_Words\"]/DFusers[\"Total_Posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8cccb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DFusers[DFusers[\"Total_Posts\"]>10].sort_values(\"Total_Words\",ascending=[False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3821184",
   "metadata": {},
   "source": [
    "### Categorizing users based on the \"effort\" of their posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d263c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PosterType=[]\n",
    "LowEffort=20\n",
    "MidEffort=40\n",
    "i=-1\n",
    "for user in UniqueUsersList:\n",
    "    i=i+1\n",
    "    if DFusers[DFusers[\"User_Name\"]==user][\"Average_Words\"][i]<LowEffort:\n",
    "        PosterType.append(\"Low effort poster\")\n",
    "    else:\n",
    "            if DFusers[DFusers[\"User_Name\"]==user][\"Average_Words\"][i]<MidEffort:\n",
    "                PosterType.append(\"Low effort poster\")\n",
    "            else:\n",
    "                    PosterType.append(\"High effort poster\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFusers[\"Poster_Type\"]=PosterType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f8460",
   "metadata": {},
   "source": [
    "## Plotting total posts and total words, we can see an extreme outlier in the writer, as not only he has more posts, but way more words as well. This makes sense as he is the one writing the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404ea9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(DFusers[\"Total_Posts\"],DFusers[\"Total_Words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff846af",
   "metadata": {},
   "source": [
    "### Here we are calculating the largest time a poster spent without posting in the thread. This only compares within the period he was active, that is \"Start date\"=FirstPostDate, \"EndDate\"=LastPostDate, so someone with only one post will have a gap of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps=[]\n",
    "for user in UniqueUsersList:\n",
    "  gap=timedelta()\n",
    "  for i in range(UserData[\"Total_Posts\"][UserData[\"User_Name\"].index(user)]-1):\n",
    "      gaptemp=PostsDictionary[\"Date\"][find_nth_occurrence(PostsDictionary[\"User_Name\"], user ,i+2)]-PostsDictionary[\"Date\"][find_nth_occurrence(PostsDictionary[\"User_Name\"], user ,i+1)]\n",
    "      if gaptemp>gap:\n",
    "        gap=gaptemp\n",
    "        \n",
    "  # usergap=PostsDictionary[\"Date\"][rindex(PostsDictionary[\"User_Name\"],user)]-PostsDictionary[\"Date\"][PostsDictionary[\"User_Name\"].index(user)]\n",
    "#  print(user + \"'s largest gap is \" + str(gap.days)+\" days\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096b7e8",
   "metadata": {},
   "source": [
    "### As I mentioned before the forum lacks a good search functionality. And as the game has been running for so many years, and has over 7000 pages of posts it's hard to find what you are looking for.\n",
    "\n",
    "### The creator of the game requested I try to create a unified word document with his posts for ease of search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21afda",
   "metadata": {},
   "source": [
    "### Creating a list of the clean text of all of his posts, I'm scraping a link that specifically only includes his post, it's more efficient than scraping all the thread and using an \"if\" to see if it should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPosts=[]\n",
    "for i in range(10):\n",
    "    j=i+1\n",
    "    print(\"Scraping page \"+str(j))\n",
    "    Response=requests.get(\"https://forums.somethingawful.com/showthread.php?threadid=3550307&userid=129241&perpage=40&pagenumber=\"+str(j))\n",
    "    pageTemp=BeautifulSoup(Response.text)\n",
    "    for k in range(postsperpage):\n",
    "        username=str(pageTemp.find_all(\"td\",class_=\"userinfo\")[k].find(\"dt\"))\n",
    "        usernameclean=username[username.find(\">\")+1:username.find(\"<\",1)]\n",
    "        if usernameclean!=\"Adbot\":\n",
    "            text=str(pageTemp.find_all(\"td\",class_=\"postbody\")[k])\n",
    "            text=text.replace(\"\\n\",\"\")\n",
    "            text=cleanhtml(text)\n",
    "            OPPosts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c3015",
   "metadata": {},
   "source": [
    "## Creating the document with the information, it can now be easily searched in Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diogposts = docx.Document()\n",
    "for i in range(len(OPPosts)):\n",
    "    Diogposts.add_paragraph(OPPosts[i])\n",
    "    Diogposts.add_paragraph(\"POST FINISHED\")\n",
    "    Diogposts.add_paragraph(\" \")\n",
    "    Diogposts.add_paragraph(\" \")\n",
    "    Diogposts.add_paragraph(\" \")\n",
    "    Diogposts.add_paragraph(\"POST BEGINS\")\n",
    "Diogposts.save(r\"C:\\Users\\Octavio Ciappina\\Documents\\Python\\MBA\\text.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04c83f",
   "metadata": {},
   "source": [
    "## exporting to excel for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cdbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFusers.to_excel(\"UserData.xlsx\")\n",
    "DFposts.to_excel(\"PostsData.xlsx\") #this is slightly heavy for a excel file, specially if i scrape the full thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95e71a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "postsperpage=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1281eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userScraping(thread):\n",
    "    gap=timedelta()   \n",
    "    response=requests.get(thread+\"1\")\n",
    "    totalwords=0\n",
    "    UserPosts={\"User_Name\":[],\"Date\":[],\"Post_Content\":[],\"Post_Words\":[]}\n",
    "    MainPage=BeautifulSoup(response.text)\n",
    "    LastpageLink=MainPage.find(\"a\",title=\"Last page\")\n",
    "    LastPage=str(LastpageLink)\n",
    "    LastPageNumber=int(LastPage[LastPage.rfind(\"=\")+13:LastPage.rfind(\" \")])\n",
    "    for i in range(LastPageNumber-1):\n",
    "        j=i+1\n",
    "        print(\"Scraping page \"+str(j))\n",
    "        Response=requests.get(thread+str(j))\n",
    "        pageTemp=BeautifulSoup(Response.text)\n",
    "        for k in range(postsperpage):\n",
    "            username=str(pageTemp.find_all(\"td\",class_=\"userinfo\")[k].find(\"dt\"))\n",
    "            usernameclean=username[username.find(\">\")+1:username.find(\"<\",1)]\n",
    "            if usernameclean!=\"Adbot\": #name of the user that posts the ads previously mentioned\n",
    "                UserPosts[\"User_Name\"].append(usernameclean)\n",
    "                date=str(pageTemp.find_all(\"td\",class_=\"postdate\")[k])\n",
    "                dateofpost=pd.to_datetime(str(date[date.rfind(\"a>\")+3:date.rfind(\"a>\")+21]))\n",
    "                UserPosts[\"Date\"].append(dateofpost)\n",
    "                text=str(pageTemp.find_all(\"td\",class_=\"postbody\")[k])\n",
    "                text=cleanhtml(text)\n",
    "                text=text.replace(\"\\n\",\"\")\n",
    "                UserPosts[\"Post_Content\"].append(text)\n",
    "                UserPosts[\"Post_Words\"].append(len(text.split()))    \n",
    "    for i in range(len(UserPosts[\"Date\"])-1):\n",
    "        gaptemp=UserPosts[\"Date\"][i+1]-UserPosts[\"Date\"][i]\n",
    "        if gaptemp>gap:\n",
    "            gap=gaptemp\n",
    "    totalwords=sum(UserPosts[\"Post_Words\"])\n",
    "    averagewords=totalwords/len(UserPosts[\"Date\"])\n",
    "    timeactive=max(UserPosts[\"Date\"])-UserPosts[\"Date\"][0]\n",
    "    daysactive=timeactive.days\n",
    "    postrate=len(UserPosts[\"Date\"])/daysactive\n",
    "    return(usernameclean + \" has \"+str(len(UserPosts[\"Date\"]))+\" posts. His posts rate is \"+str(postrate)+\" posts per day he has been active. The longest he has not posted is \"+str(gap.days)+\" days. He has written a total of \"+str(totalwords)+\" words. Average words per post is \"+ str(averagewords)+\".\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0084734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "Scraping page 40\n",
      "Scraping page 41\n",
      "Scraping page 42\n",
      "Scraping page 43\n",
      "Scraping page 44\n",
      "Scraping page 45\n",
      "Scraping page 46\n",
      "Scraping page 47\n",
      "Scraping page 48\n",
      "Scraping page 49\n",
      "Scraping page 50\n",
      "Scraping page 51\n",
      "Scraping page 52\n",
      "Scraping page 53\n",
      "Scraping page 54\n",
      "Scraping page 55\n",
      "Scraping page 56\n",
      "Scraping page 57\n",
      "Scraping page 58\n",
      "Scraping page 59\n",
      "Scraping page 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Swedish Thaumocracy has 2400 posts. His posts rate is 0.7970773829292593 posts per day he has been active. The longest he has not posted is 161 days. He has written a total of 200258 words. Average words per post is 83.44083333333333.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userScraping(\"https://forums.somethingawful.com/showthread.php?threadid=3550307&userid=99457&perpage=40&pagenumber=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    j=i+1\n",
    "    print(\"Scraping page \"+str(j))\n",
    "    Response=requests.get(\"https://forums.somethingawful.com/showthread.php?threadid=3643994&userid=0&perpage=40&pagenumber=\"+str(j))\n",
    "    pageTemp=BeautifulSoup(Response.text)\n",
    "    for k in range(postsperpage):\n",
    "        username=str(pageTemp.find_all(\"td\",class_=\"userinfo\")[k].find(\"dt\"))\n",
    "        usernameclean=username[username.find(\">\")+1:username.find(\"<\",1)]\n",
    "        if usernameclean!=\"Adbot\": #name of the user that posts the ads previously mentioned\n",
    "            PostsDictionary[\"User_Name\"].append(usernameclean)\n",
    "            date=str(pageTemp.find_all(\"td\",class_=\"postdate\")[k])\n",
    "            dateofpost=pd.to_datetime(str(date[date.rfind(\"a>\")+3:date.rfind(\"a>\")+21]))\n",
    "            PostsDictionary[\"Date\"].append(dateofpost)\n",
    "            text=str(pageTemp.find_all(\"td\",class_=\"postbody\")[k])\n",
    "            text=cleanhtml(text)\n",
    "            text=text.replace(\"\\n\",\"\")\n",
    "            PostsDictionary[\"Post_Content\"].append(text)\n",
    "            PostsDictionary[\"Post_Words\"].append(len(text.split()))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
